WEBVTT

00:00.000 --> 00:05.080
Welcome to the Deep Dive, the show where we take a stack of sources, articles, research papers,

00:05.260 --> 00:09.860
your own notes, and really extract the most important nuggets of knowledge or insight just

00:09.860 --> 00:15.860
for you. Today we're plunging into a topic that honestly affects anyone who's ever tried to build

00:15.860 --> 00:20.440
something digital with another human being. Think about your last collaborative project.

00:20.820 --> 00:25.540
Maybe you're sharing code on GitHub, maybe co-editing a research paper, or, you know,

00:25.580 --> 00:29.720
even just working on a shared spreadsheet. It seems seamless, right? You pull the latest version,

00:29.720 --> 00:34.440
you make your changes, you push, you merge. But often, if you're being honest, it's kind of

00:34.440 --> 00:40.200
plagued by frustrating merge conflicts, these inexplicable errors, or just this nagging feeling

00:40.200 --> 00:40.960
of disconnect.

00:45.600 --> 00:50.300
Yeah, that everyday experience of friction, that fundamental miscommunication you find in digital

00:50.300 --> 00:55.540
collaboration, that's exactly what this deep dive is really about. We're exploring a pretty groundbreaking

00:55.540 --> 01:02.340
proposal that fundamentally rethinks how we build and share digital artifacts. It's about moving

01:02.340 --> 01:08.600
beyond the current syntactic approach, you know, where our tools only see the text, the lines of code

01:08.600 --> 01:14.000
that surface of us, to a truly semantic one, where they grasp the meaning, the intent, the underlying

01:14.000 --> 01:16.380
conceptual relationships behind those changes.

01:16.380 --> 01:21.980
And when we say digital artifacts, we're talking about basically everything, right? From code and complex data

01:21.980 --> 01:26.380
structures, all the way to scientific models, and even abstract philosophical theories. This is a visual-

01:26.380 --> 01:26.980
This is a visual-

01:26.980 --> 01:33.380
And, yeah, that title alone kind of signals the depth we're about to get into. Our current tools, like Git and GitHub, they're fundamentally built to track files and textual edits.

01:33.380 --> 01:40.380
Simple as that. They have, well, no inherent understanding of the intent or the meaning behind those changes you're making. And this creates a cascade of problems. Problems that might show up as, uh,

01:40.380 --> 01:47.380
symbolic namespace collisions. Symbolic namespace collisions. Okay. That sounds like a very technical way to describe something else.

01:47.380 --> 01:48.380
And, yeah, that's a visual-

01:48.380 --> 02:08.380
And, yeah, that title alone kind of signals the depth we're about to get into. Our current tools, like Git and GitHub, they're fundamentally built to track files and textual edits. Simple as that. They have, well, no inherent understanding of the intent or the meaning behind those changes you're making. And this creates a cascade of problems. Problems that might show up as, uh, symbolic namespace collisions. Symbolic namespace collisions? Okay. That sounds like a very technical way to describe something else.

02:08.380 --> 02:15.380
incredibly frustrating in fact is, what does that actually mean for someone trying to get their work done on a team project?

02:15.380 --> 02:38.360
Right. It means that the system might see two different pieces of code using the same name for, say, a variable or a function. Even if they operate in completely different contexts or have totally the same purposes. So when you try to merge it, the tool flags a conflict because the name is Clash. Even if the underlying meaning or intent of the code is purposely compatible, or, you know, the

02:38.360 --> 02:50.360
the opposite can happen to. Two pieces of code that look similar textually might mean wildly different things, and they can get merged inappropriately. This is what the authors call epistemic fragmentation when you use forks.

02:50.360 --> 02:53.360
Forks, right? Those divergent branches teams create.

02:53.360 --> 03:05.360
Exactly. Instead of being conceptually composable units, ways of explaining.

03:05.360 --> 03:17.360
So, to put it in a really practical, relatable way for maybe someone listening,

03:17.360 --> 03:29.360
get nosy change the line 57 of file by pi. Okay, great. But you're saying it has absolutely no idea if that change optimizes a neural network's loss function to make a model more accurate.

03:29.360 --> 03:33.360
Or if it just refactor the type of clarification for clarity in a completely unrelated way of the system.

03:33.360 --> 03:38.360
The meaning of the change, the developer's intent, is completely lost in the system's view.

03:38.360 --> 03:43.360
It's simply a very sophisticated text editor that tracks changes, not concepts.

03:43.360 --> 03:47.360
And that fundamental gap is exactly where the semantic infrastructure framework steps in.

03:47.360 --> 03:50.360
It proposes a really radical shift in perspective.

03:50.360 --> 03:54.360
Treating computational modules not as mere files or blocks of text.

03:54.360 --> 04:11.360
Yes, because this isn't just some abstract philosophical idea.

04:11.360 --> 04:16.360
It's grounded in some truly powerful and maybe a bit intimidating mathematical concepts.

04:16.360 --> 04:26.360
We're talking about the relativistic scale of the vector plenum, RSVP, theory both mathematical physics plus higher category theory, and sheath theory.

04:26.360 --> 04:30.360
My head is already spinning a little, I'll admit, but that's why we're here today.

04:30.360 --> 04:32.360
For you listening, we're going to untack it.

04:32.360 --> 04:40.360
Indeed. And your challenge is fascinating to trace how these incredibly complex ideas actually evolved across the different drafts of this monograph.

04:40.360 --> 04:41.360
Yeah.

04:41.360 --> 04:47.360
I wanted a general overview, naturally, but crucially, you asked us to explain the most difficult concepts in an accessible way.

04:47.360 --> 04:54.360
So, what does this all mean for you as a listener?

04:54.360 --> 05:10.360
Whether you're prepping for a meeting, trying to get up to speed on a cutting edge field, or maybe you're just insanely curious about the future, or something like that.

05:10.360 --> 05:27.360
You have to get a shortcut. A shortcut to being well informed on a topic that could fundamentally redefine how we build, share, and truly collaborate digitally.

05:27.360 --> 05:38.360
You'll hopefully gain a unique understanding of a system that aims to eliminate that frustration of conflicting intents, letting you focus on the meaning of your work, not just its syntax.

05:38.360 --> 05:40.360
Okay, let's dig in.

05:40.360 --> 05:48.360
Alright, so our deep dive into how this framework evolved, it really has to begin with this powerful critique, which seems especially prominent in Draft 03 and Draft...

05:48.360 --> 06:05.360
That's a great question, actually, because illusion implies something deeper than just a flaw.

06:05.360 --> 06:11.360
It suggests a foundational misunderstanding of how complex systems actually go here, how they hold together.

06:11.360 --> 06:18.360
As those drafts, and later ones, like Draft 06, have really meticulously explained, GitHub creates an illusion of coherence.

06:18.360 --> 06:24.360
It presents itself as this unified, seamless namespace for projects and contributors, right?

06:24.360 --> 06:27.360
It looks like a pristine environment for collaboration.

06:27.360 --> 06:36.360
However, underneath that polished surface, GitHub operates fundamentally as a permissioned layer over traditional file systems and symbolic version control.

06:36.360 --> 06:39.360
Okay, a permissioned layer, what does that mean in practice?

06:39.360 --> 06:41.360
It means its primary function isn't really to...

06:41.360 --> 06:56.360
So, it's about controlling access and tracking lines of text, but not the actual deep meaning of the conceptual structure.

06:56.360 --> 07:13.360
That sounds subtle, maybe, but the monograph really positions it as a truly fundamental problem.

07:13.360 --> 07:19.360
Can you give us a really concrete example of how this illusion plays out in, say, real-world collaborative development?

07:19.360 --> 07:22.360
I know there's an antidote they use.

07:22.360 --> 07:26.360
Absolutely, and yet, that antidote becomes more detailed and consistent from draftos.

07:26.360 --> 07:28.360
It gets along with desiccated illustrations.

07:28.360 --> 07:31.360
So, imagine a large interdisciplinary research team.

07:31.360 --> 07:34.360
They're intensely working on a complex climate prediction model.

07:34.360 --> 07:35.360
Huge project.

07:35.360 --> 07:37.360
Hundreds of thousands of lines of code.

07:37.360 --> 07:42.360
You've got one senior researcher maybe dedicated to optimizing a specific law function to reduce...

07:42.360 --> 08:03.360
...to reduce predi-

08:03.360 --> 08:06.360
...generalizability.

08:06.360 --> 08:11.360
Okay, three different experts working on different conceptual parts of the same model.

08:11.360 --> 08:12.360
Exactly.

08:12.360 --> 08:17.360
Now, in a GitHub environment, what you typically see are three sets of textual devs,

08:17.360 --> 08:22.360
changes to lines of code within various Python files or maybe C++ files.

08:22.360 --> 08:26.360
These changes, even though they're profoundly semantically compatible at a higher level,

08:26.360 --> 08:30.360
I mean, they all contribute to a better, more robust, more accurate model,

08:30.360 --> 08:34.360
they can easily lead to syntactic conflicts in shared files.

08:34.360 --> 08:38.360
GitHub, with a file-centered text-based view,

08:38.360 --> 08:44.360
is simply unable to recognize that the entropy reduction achieved by optimizing the loss function

08:44.360 --> 08:48.360
actually complements the coherence enhancements from the data.

09:04.360 --> 09:12.360
...conflicts just based on word choice, ignoring the actual message.

09:12.360 --> 09:14.360
That makes so much sense.

09:14.360 --> 09:18.360
So it's not just a minor inconvenience or some frustrating speed bump in the workflow,

09:18.360 --> 09:24.360
it's as draft 03 and 04 so pointedly state a representational error.

09:24.360 --> 09:27.360
The core issue is that Git, and by extension GitHub,

09:27.360 --> 09:31.360
is operating under this flawed assumption that text equals meaning.

09:31.360 --> 09:46.360
Correctness in those domains isn't simply about syntactic compatibility

09:46.360 --> 09:48.360
or whether a line of code compires,

09:48.360 --> 09:51.360
it's about whether the ideas align and compose correctly.

09:51.360 --> 10:05.360
Exactly. And this problem goes beyond just...

10:05.360 --> 10:10.360
Yeah. It means that conceptual divergences, like different approaches to solving the same problem

10:10.360 --> 10:12.360
or different ways of evolving a model,

10:12.360 --> 10:16.360
they become the structural break in the system's understanding.

10:16.360 --> 10:19.360
They aren't treated as composable languages that could potentially be

10:19.360 --> 10:21.360
harmoniously integrated later.

10:21.360 --> 10:24.360
These force literally fragment the knowledge contained within the project.

10:24.360 --> 10:28.360
is in this fundamental disconnect to the inability of our current tool

10:28.360 --> 10:31.360
to understand the meaning of our computational artifacts

10:31.360 --> 10:33.360
and how that meaning evolved.

10:33.360 --> 10:37.360
That's the core motivation for the entire semantic infrastructure framework they're proposing.

10:37.360 --> 10:41.360
And this is where it really hits home for you, your listener.

10:41.360 --> 10:43.360
Have you ever been stuck trying to merge code

10:43.360 --> 10:46.360
knowing exactly what you wanted to achieve with your changes,

10:46.360 --> 10:49.360
but the tool just sees conflicting lines of text,

10:49.360 --> 10:52.360
forcing you into that tedious manual resolution process

10:52.360 --> 10:55.360
that feels like you're just fighting the system?

10:55.360 --> 10:58.360
This entire framework, this whole deep dive,

10:58.360 --> 11:01.360
is ultimately aimed at solving that fundamental disconnect.

11:01.360 --> 11:04.360
It's about building a system that understands your intent,

11:04.360 --> 11:06.360
not just your keystrokes.

11:06.360 --> 11:09.360
This is the central problem the authors are challenging,

11:09.360 --> 11:12.360
and their proposed solution is, well, as elegant as it is complex.

11:12.360 --> 11:14.360
Let's get into that solution.

11:14.360 --> 11:18.360
Okay, so if the fundamental problem is that our current systems don't understand meaning,

11:18.360 --> 11:19.360
then...

11:36.360 --> 11:39.360
Yeah, the big idea, and you can see this introduced in draft 01,

11:39.360 --> 11:43.360
and it is significantly expanded upon in later drafts, especially VO3 and VO4,

11:43.360 --> 11:47.360
is to model computation, not as static operations on data,

11:47.360 --> 11:52.360
but as dynamic interactions of scalar coherence fields, vector inference flows bys,

11:52.360 --> 11:57.360
and entropy fields as over space-time manifolds M equals VR3.

11:57.360 --> 11:58.360
Whoa, okay.

11:58.360 --> 11:59.360
It's a truly profound shift,

11:59.360 --> 12:02.360
through essentially treating computation itself as a kind of physics,

12:02.360 --> 12:06.360
a flow of semantic energy, where meaning isn't just present...

12:06.360 --> 12:09.360
But it moves, it changes, it has measurable properties,

12:09.360 --> 12:12.360
almost like particles or waves in the physical universe.

12:12.360 --> 12:14.360
Okay, space-time manifold and fields.

12:14.360 --> 12:17.360
That already sounds pretty deep and maybe a bit...

12:26.360 --> 12:27.360
Okay, let's break them down.

12:27.360 --> 12:28.360
It helps use analogies.

12:28.360 --> 12:31.360
First, the scalar coherence field.

12:31.360 --> 12:35.360
This represents semantic alignment, or conceptual coherence.

12:35.360 --> 12:40.360
Think of it as a measure of how aligned, or unified, or simply how meaningful

12:40.360 --> 12:44.360
a piece of computational state or a model is within its broader theoretical domain.

12:44.360 --> 12:49.360
Like, imagine a perfectly clear, calm lake that's a system with high irony.

12:49.360 --> 12:52.360
Everything is aligned, understood, coherent.

12:52.360 --> 12:56.360
And that climate model anecdote we talked about, which by the way becomes a really consistent

12:56.360 --> 13:01.360
and detailed example from draft 11 onwards, would encode something like model accuracy,

13:01.360 --> 13:05.360
or the overall conceptual coherence of the model's components working together.

13:05.360 --> 13:07.360
Higher A means a more consistent...

13:07.360 --> 13:16.360
Alright, next is the vector inference flow.

13:16.360 --> 13:19.360
This field directs updates to semantic states.

13:19.360 --> 13:24.360
You can think of it as analogous to attentional shifts or dependency traversals within the computational system.

13:24.360 --> 13:30.360
It describes how meaning moves, or how information propagates and transforms into the system.

13:30.360 --> 13:38.360
So, if a calm lake has a small directed current flowing through it, such a V-field guiding, say, a leaf representing data,

13:38.360 --> 13:40.360
or a computational process in a specific direction.

13:40.360 --> 13:45.360
For our climate model, the V-field would direct the data flow through the various processing stages,

13:45.360 --> 13:48.360
from raw sensor data, through analysis to simulation,

13:48.360 --> 13:52.360
and it would also model how different components influence each other's semantic state.

13:52.360 --> 13:53.360
Got it.

13:53.360 --> 13:54.360
Vice-size coherent.

13:54.360 --> 13:59.360
And finally, the entropy field.

13:59.360 --> 14:01.360
S.

14:01.360 --> 14:08.360
This quantifies uncertainty, prediction error, or, maybe more broadly, the thermodynamic cost of computation.

14:08.360 --> 14:12.360
It's a measure of disorder, or unpredictability in the semantic system.

14:12.360 --> 14:17.360
In that climate model scenario, the S-field would quantify things like prediction variance,

14:17.360 --> 14:19.360
or the overall ambiguity in the model's output,

14:19.360 --> 14:24.360
or maybe even the computational effort required to achieve a certain level of coherence.

14:24.360 --> 14:28.360
So if a calm lake suddenly becomes turbulent and choppy, that's high S.

14:28.360 --> 14:33.360
The path of any leaf on its surface becomes uncertain, unpredictable, reflecting high semantic disorder.

14:33.360 --> 14:36.360
That lake analogy really helps bring it down to Earth.

14:36.360 --> 14:38.360
Okay, so a T is calmness.

14:38.360 --> 14:39.360
V is current.

14:39.360 --> 14:40.360
S is...

14:40.360 --> 14:41.360
That's a great question.

14:41.360 --> 14:51.360
Because interplay is the key to the dynamism of the system.

14:51.360 --> 15:00.360
If you have high IFN, so high coherence, but also high S, high entropy, it might mean you have a system that is theoretically sound,

15:00.360 --> 15:02.360
and very well defined perhaps.

15:02.360 --> 15:06.360
But in practice, its behavior is unpredictable or noisy.

15:06.360 --> 15:11.360
Maybe it's due to external factors or inherent complexity in the problem it's trying to solve.

15:11.360 --> 15:18.360
Think of a brilliant, perfectly defined algorithm that is running on extremely messy, unpredictable, real world data.

15:18.360 --> 15:23.360
The algorithm itself is coherent, high A, but the result is chaotic, high S.

15:23.360 --> 15:24.360
Okay, that makes sense.

15:24.360 --> 15:30.360
Conversely, low A and low S would maybe represent a system that is very predictable.

15:30.360 --> 15:50.360
That's fascinating, and these fields aren't just conceptual metaphors, this is where the serious math comes in.

15:50.360 --> 15:54.360
They are governed by very specific mathematical laws.

15:54.360 --> 16:01.360
You mentioned you can really see the evolution of this rigor across the drafts, right, from just cruelly mentions to full formalization.

16:01.360 --> 16:02.360
That's exactly right.

16:02.360 --> 16:14.360
Draft 0-1 mentions them, but it's really draft 0-3 and 0-4, where the full ethos stochastic differential equations, or SVEs, for A, V, and S, are explicitly introduced and defined.

16:14.360 --> 16:18.360
Now, we don't need to get lost in every single Greek letter and symbol here.

16:18.360 --> 16:19.360
T-zone.

16:19.360 --> 16:20.360
Right.

16:20.360 --> 16:25.360
But think of these equations as the underlying physics that govern how meaning behaves in this proposed digital universe.

16:25.360 --> 16:32.360
They look complex on-

16:32.360 --> 16:39.360
The equations essentially define the dynamic evolution of each field, showing how they influence each other constantly.

16:39.360 --> 16:47.360
For the data equation, that's the change in coherence over time, it describes how semantic coherence spreads, or maybe dissipates, kind of like heat, that's the diffusing term.

16:47.360 --> 17:06.360
It also shows how it's directly influenced by the inference flow, showing how directed computation actively shapes meaning.

17:06.360 --> 17:12.360
And it shows how entropy S might, coupled to it, influence in coherence.

17:12.360 --> 17:13.360
That's the S term.

17:13.360 --> 17:15.360
So, coherence isn't static.

17:15.360 --> 17:19.360
It's actively shaped by information flow, and it's affected by the level of disorder.

17:19.360 --> 17:20.360
Okay.

17:20.360 --> 17:23.360
So, coherence changes based on how information flows and how messy things are.

17:23.360 --> 17:24.360
What about the flow itself?

17:24.360 --> 17:25.360
V-

17:25.360 --> 17:31.360
By particularity, the change in the inference flow tells us that these flows are driven by the gradient of entropy.

17:31.360 --> 17:33.360
This is a really crucial concept.

17:33.360 --> 17:40.360
It suggests that computational processes naturally tend to move towards states of lower uncertainty or higher predictability.

17:40.360 --> 17:45.360
It's kind of like water flowing downhill, driven by a gradient and gravitational potential.

17:45.360 --> 17:51.360
Here, semantic information flows downhill towards a greater order, driven by the entropy gradient.

17:51.360 --> 18:01.360
Inference flows, meaning clear meaning can actually guide better, more efficient computational paths.

18:01.360 --> 18:03.360
And entropy S, how does that change?

18:03.360 --> 18:06.360
And for DST, the change in entropy, it links-

18:06.360 --> 18:07.360
Minimize for computation.

18:07.360 --> 18:08.360
Can you mention noise terms?

18:08.360 --> 18:21.360
Yes, all these equations also include stochastic noise terms, N-E-W.

18:21.360 --> 18:36.360
This isn't just mathematical complexity for its own sake.

18:36.360 --> 18:42.360
It's a crucial element that acknowledges inherent uncertainty or random fluctuations within any real semantic system.

18:42.360 --> 18:50.360
It makes the model more realistic and robust rather than assuming a perfectly deterministic, predictable universe.

18:50.360 --> 18:52.360
That's a lot of intricate interplay.

18:52.360 --> 18:56.360
It really paints a picture of meaning as this dynamic of the living thing.

18:56.360 --> 19:03.360
But what's truly interesting as you track the evolution is how the graph started to build in formal proofs for these concepts,

19:03.360 --> 19:05.360
moving beyond just defining the equations.

19:05.360 --> 19:22.360
This indicates a significant increase in the mathematical rigor and confidence in the monograph.

19:22.360 --> 19:25.360
Well, it's not academic, but it's actually crucial for practicality.

19:25.360 --> 19:30.360
This development is really significant, appearing consistently from draft 08 onwards,

19:30.360 --> 19:38.360
and then importantly, expanded with natural language explanations in D09, D11, D12, and D13, making it more accessible.

19:38.360 --> 19:43.360
Then introduced theorem 8.1, well-phoseness of RSVP-SPDE system.

19:43.360 --> 19:47.360
To understand why it matters, think of it like building a stable bridge.

19:47.360 --> 19:52.360
You need to know your calculations will always give you a solution for the bridge's design and existence.

19:52.360 --> 19:55.360
That it's the only solution for those specific initial conditions.

19:55.360 --> 19:56.360
That's uniqueness.

19:56.360 --> 20:00.360
And that if you tweak one tiny bolt just a little, or the wind shifts slightly,

20:00.360 --> 20:02.360
the whole bridge doesn't suddenly collapse unpredictably.

20:02.360 --> 20:21.360
We'll have predictable semantic outcomes.

20:21.360 --> 20:22.360
Exactly that.

20:22.360 --> 20:26.360
The natural language explanation provided in those later drafts is very helpful here.

20:26.360 --> 20:31.360
It ensures the RSVP feels evolved smoothly, like a roller flowing without sudden disruptions.

20:31.360 --> 20:36.360
That directly addresses the idea of predictable and stable semantic evolution.

20:36.360 --> 20:44.360
Furthermore, the concept of a Conserved Energy Functional, ET, means that the systemized stability is akin to a balanced ecosystem.

20:44.360 --> 20:56.360
This guarantees that coherence, inference, and uncertainty remain in harmony on average, enabling reliable semantic computation.

20:56.360 --> 20:58.360
It's not designed to be a chaotic system.

20:58.360 --> 21:04.360
It's designed for predictability and stability, even in a world where meaning is dynamic and constantly evolving.

21:04.360 --> 21:06.360
This rigorous proof transforms RSVP from...

21:06.360 --> 21:14.360
This rigorous proof transforms RSVP from...

21:14.360 --> 21:31.360
...reduce semantic compression in ways we never thought possible, with simple text-based tools.

21:31.360 --> 21:34.360
It's like having a semantic force field keeping your project coherent.

21:34.360 --> 21:36.360
That's a critical takeaway, then.

21:36.360 --> 21:38.360
This isn't just a philosophical pipe dream.

21:38.360 --> 21:41.360
It's intended as a mathematically proven framework.

21:41.360 --> 21:47.360
It's designed to ensure predictability and stability in a system where meaning is inherently dynamic.

21:47.360 --> 21:53.360
That gives confidence that a system built on these principles wouldn't just conceptually work, but could actually function reliably.

21:53.360 --> 22:06.360
Okay, so if RSVP is the underlying physics of this new semantic space, describing how meaning flows and interacts, what are the actual things we're building, sharing, and merging in this infrastructure?

22:06.360 --> 22:09.360
What exactly is a semantic module in this context?

22:09.360 --> 22:12.360
Is it just a fancier name for a file or a function?

22:12.360 --> 22:14.360
That's a vital distinction to make.

22:14.360 --> 22:15.360
A semantic module which is the fundamental...

22:15.360 --> 22:41.360
Okay, M equals FAD. Let's write that down. What are each of those components doing?

22:41.360 --> 22:46.360
But let's unpack them, as they're crucial to understanding why a module is so different.

22:46.360 --> 22:49.360
First, F is a finite set of function hashes.

22:49.360 --> 22:58.360
This uniquely identifies the computational operations, whether it's a code fragment, a whole algorithm, or some specific piece of logic using content-based addressing.

22:58.360 --> 23:02.360
Think of it like a digital fingerprint for the actual code's behavior, not just its text.

23:02.360 --> 23:10.360
If two pieces of code do the exact same thing semantically, even if written slightly differently, their F hash would reflect that functional equivalence.

23:10.360 --> 23:14.360
Okay, so F identifies what it does, the computation itself. What's that?

23:14.360 --> 23:18.360
Second, sigma is a set of semantic type annotations.

23:18.360 --> 23:21.360
And this is where the framework really departs from traditional systems.

23:21.360 --> 23:25.360
This goes way beyond simple data types like integer or string.

23:25.360 --> 23:30.360
Instead, it explicitly specifies the module's role within a broader theoretical domain.

23:30.360 --> 23:46.360
For instance, HMI might tell us that this module is specifically an RSVP entropy field calculator, or maybe it's a semantic information theory, SIT memory operator, or perhaps a component of a compositional monoidal COM framework designed for strictly ordered interactions.

23:46.360 --> 23:50.360
This clearly defines what the code means conceptually, what kind of entity it represents.

23:50.360 --> 23:54.360
This is what enables intelligent composition based on conceptual compatibility, not just textual.

23:54.360 --> 24:18.360
...relationship and detailing the flow of meaning within the module itself.

24:18.360 --> 24:20.360
Dependencies, got it.

24:20.360 --> 24:23.360
And the last one, segue letter 4.

24:23.360 --> 24:28.360
And finally, oh say, this represents an entropy flow morphism.

24:28.360 --> 24:33.360
This is the really crucial link that ties the module directly into that RSVP planner we just discussed.

24:33.360 --> 24:35.360
It's a precise mathematical mapping.

24:35.360 --> 24:49.360
It takes the module's semantic annotations and maps them to the space S of semantic roles, which are themselves parameterized by the dynamic RSVP fields, coherence, inference flow, and entropy S.

24:49.360 --> 24:55.360
This is essentially where the module's defined meaning gets embedded into the dynamic and tropic space of RSVP.

24:55.360 --> 25:00.360
It's how the system understands precisely how the specific module contributes to and is effective.

25:00.360 --> 25:26.360
Like a truly living, breathing, computational entity almost aware of its own purpose and context within a broader system.

25:26.360 --> 25:30.360
Precisely. That's a great way to put it.

25:30.360 --> 25:38.360
As needed consistently from graph 04 all the way through graph 13, each module is described as a condensate of meaning, a packet of structured entropy.

25:38.360 --> 25:41.360
It's not merely a passive collection of data.

25:41.360 --> 25:49.360
It's a self-contained unit that carries its own semantic context and actively interacts with the broader semantic environment through its defined and tropic flows.

25:49.360 --> 25:57.360
This means it's not a static artifact, but an active participant in the ongoing construction and overmotion of meaning within the system.

25:57.360 --> 26:02.360
And building on that idea of dynamism, the drafts also talk about code is structured and tropic flow.

26:02.360 --> 26:03.360
That takes the idea even further.

26:03.360 --> 26:05.360
What exactly is happening there?

26:05.360 --> 26:07.360
How does code itself become a flow?

26:07.360 --> 26:08.360
Right.

26:08.360 --> 26:15.360
Graph 0304 introduces a really powerful idea that a function within a module isn't just a static artifact waiting to be executed.

26:15.360 --> 26:24.360
Instead, it's conceptualized as a morphism, a transformation that actively induces a change in the A field, the coherence field, over time as it runs.

26:24.360 --> 26:25.360
Think of it like this.

26:25.360 --> 26:28.360
When a function executes, it doesn't just produce an output value.

26:28.360 --> 26:38.360
It actively transforms the semantic state of the system around it, potentially increasing or decreasing coherence in that local region of the semantic space.

26:38.360 --> 26:47.360
The equation XD plus is CO8AT formalizes this, showing how the inference flow each associated with a function of modifies the existing coherence field A01.

26:47.360 --> 26:48.360
So running code actually changed-

26:48.360 --> 26:49.360
Yes, exactly.

26:49.360 --> 27:06.360
And concurrently, the entropy field S evolves to reflect the computational cost, or perhaps the information gain associated with that specific transformation.

27:06.360 --> 27:10.360
Every computation consumes energy and generates information, right?

27:10.360 --> 27:13.360
This framework attempts to quantify that at a semantic level.

27:13.360 --> 27:23.360
This remains computation from being about manipulating static tactics artifacts to being a dynamic process that continually impacts and reshaves the semantic landscape.

27:23.360 --> 27:27.360
And this dynamism is what enables truly semantic composition and merging.

27:27.360 --> 27:36.360
Because the system understands that when code executes, it's not just producing an output, it's actively transforming the underlying semantic coherence and entropic state of the computational universe.

27:36.360 --> 27:38.360
It's a paradigm shift.

27:38.360 --> 27:47.360
That is truly a profound shift in thinking.

27:47.360 --> 27:50.360
It means every line of code isn't just an instruction.

27:50.360 --> 27:55.360
It's an action that leaves a distinct semantic footprint, altering the knowledge landscape around it.

27:55.360 --> 28:08.360
This could potentially revolutionize how we debug, optimize, and even design software, moving beyond just performance metrics to understanding things like semantic efficiency or conceptual clarity.

28:08.360 --> 28:18.360
Okay, so we have RSVP as the fundamental physics, and these semantic modules as the dynamic units within that physics, constantly evolving and interacting.

28:18.360 --> 28:23.360
Now this entire framework, as we hinted, leans heavily on some really advanced mathematical concepts.

28:23.360 --> 28:26.360
These provide the necessary rigor and expressive power to actually...

28:26.360 --> 28:44.360
Right, category 3 is introduced very early on.

28:44.360 --> 28:49.360
Drafts of 1, for instance, explicitly mentions the category C of semantic modules.

28:49.360 --> 28:54.360
That immediately tells us that modules aren't just seen as a collection.

28:54.360 --> 28:57.360
They are defined relationships and transformations between them.

28:57.360 --> 29:01.360
However, the level of sophistication ramps up pretty quickly in the draft.

29:01.360 --> 29:09.360
By draft 08, the abstract explicitly mentions using higher category theory and describes an iterative category of semantic modules.

29:09.360 --> 29:14.360
This evolution reflects a deepening commitment to using these more advanced mathematical tools,

29:14.360 --> 29:19.360
likely because they found they were required to accurately model the real nuances of meaning and composition.

29:19.360 --> 29:20.360
Okay.

29:20.360 --> 29:21.360
Category.

29:21.360 --> 29:22.360
Why ERA?

29:22.360 --> 29:23.360
What does higher mean here?

29:23.360 --> 29:24.360
Beyond...

29:24.360 --> 29:26.360
That's a key point.

29:26.360 --> 29:32.360
Referencing the prerequisite section in DOA and later drafts.

29:32.360 --> 29:38.360
A standard category, as you said, consists of objects like our semantic modules and morphisms,

29:38.360 --> 29:41.360
which are the functions or transformations between those objects.

29:41.360 --> 29:42.360
Think of a simple diagram.

29:42.360 --> 29:44.360
Dots connected by arrows.

29:44.360 --> 29:48.360
Now, in your category, it extends this concept by introducing higher morphisms.

29:48.360 --> 29:51.360
So you don't just have functions between objects, one morphism.

29:51.360 --> 29:56.360
You have two morphisms between the functions, three morphisms between two morphisms, and so on,

29:56.360 --> 29:58.360
potentially up to infinity.

29:58.360 --> 30:03.360
These higher structures are often modeled using complex mathematical machinery like simplicial sets.

30:03.360 --> 30:08.360
So it's about modeling not just the transformations themselves, but also transformations...

30:08.360 --> 30:09.360
Yes, exactly.

30:09.360 --> 30:10.360
That's a great analogy.

30:10.360 --> 30:23.360
This allows for a much more nuanced and precise understanding of what they call higher coherence.

30:23.360 --> 30:26.360
It's not just about whether things connect, do the types match, but whether the ways they

30:26.360 --> 30:30.360
connect are themselves consistent, and how those relationships can deform or evolve without

30:30.360 --> 30:31.360
breaking the core underlying meaning.

30:31.360 --> 30:32.360
In this framework, it allows for modeling how semantic relationships preserve those groups

30:32.360 --> 30:36.360
and how they can form or evolve without breaking the core underlying meaning.

30:36.360 --> 30:37.360
In this framework, it allows for modeling how semantic relationships preserve those groups and

30:37.360 --> 30:47.360
free-to-entropy flows, during transformations between modules.

30:47.360 --> 30:51.360
For example, when you refactor a piece of code changes internal structure, but not its external

30:51.360 --> 30:56.360
behavior, the Cheery category framework could potentially ensure that while the syntax changes significantly,

30:56.360 --> 31:01.300
For example, when you refactor a piece of code change its internal structure, but not

31:01.300 --> 31:05.440
its external behavior, the Cheery Category framework could potentially ensure that while

31:05.440 --> 31:10.280
the syntax changes significantly, the semantic intent, its VOS interactions,

31:10.280 --> 31:24.600
this means modules are always contextualized by a specific theoretical domain, like RSVP

31:24.600 --> 31:30.160
theory itself, or maybe SIT, semantic information theory for understanding information flow and

31:30.160 --> 31:35.520
its meaning content, or COM, compositional monoidal framework for systems with very destruct

31:35.520 --> 31:37.760
ordered interactions.

31:37.760 --> 31:42.360
This contextualization is what enables potentially very similar context-aware semantic translations

31:42.360 --> 31:46.720
and compositions, and ensures that a module's meaning is understood not in isolation, but

31:46.720 --> 31:49.560
always within its specific theoretical context.

31:49.560 --> 31:52.560
That extra layer of context seems absolutely critical.

31:52.560 --> 31:58.040
Okay, moving on to the next major mathematical pillar, Sheaf Theory.

31:58.040 --> 32:02.360
When did this appear in the drafts, and what specific problems does it solve within this

32:02.360 --> 32:04.360
already complex semantic framework?

32:04.360 --> 32:07.360
Why do we need sheaves if we already have these fancy categories?

32:07.360 --> 32:09.960
Sheaf Theory is mentioned as a key tool quite early on.

32:09.960 --> 32:16.320
Appearing in draft 03 and 04, its role then becomes much more explicit in draft 06 and 07,

32:16.320 --> 32:21.960
which introduced the concept of sheaves-theoretic modular gluing.

32:21.960 --> 32:26.080
The core problem it solves is ensuring local to global consistency.

32:26.080 --> 32:29.680
This is especially important when you're trying to combine different patches of information

32:29.680 --> 32:30.680
or computation.

32:30.680 --> 32:38.640
Yeah, the natural language explanation in the later draft, D-09 onward, is a pretty good hit.

32:38.640 --> 32:41.240
Think of sheave gluing like assembling a jigsaw puzzle.

32:41.240 --> 32:47.600
Each piece represents a local module or a local piece of information.

32:47.600 --> 33:00.200
The pieces fit together perfectly with their neighbors, only if they match on the shared edges.

33:00.200 --> 33:02.800
These edges represent overlapping contexts over shared dependencies.

33:02.800 --> 33:03.800
If all these local pieces are representing, say, different parts of the computational system

33:03.800 --> 33:16.800
or different contributions to a collaborative project aligned correctly on their overlap,

33:16.800 --> 33:22.400
then they form a complete coherent picture, which is the global unified module or system state.

33:22.400 --> 33:27.400
In the context of RSVP, this ensures that local contributions may be like different parts of it.

33:27.400 --> 33:48.000
It's about making absolutely sure that local changes or local components are consistent globally across the entire system,

33:48.000 --> 33:53.000
like building a reliable distributed system that can maintain a single coherent state,

33:53.000 --> 33:59.600
even though it's built from many smaller independent pieces, and Montagnac formalizes this with a group, I assume.

33:59.600 --> 34:00.600
Precisely.

34:00.600 --> 34:07.600
And yes, from draft and only onward, the authors introduced the theorem B.1, semantic coherence via sheave gluing.

34:07.600 --> 34:14.600
This provides a formal mathematical proof that if a local field, like the A, Vs, and S fields of individual modules,

34:14.600 --> 34:21.600
agree on their overlapping regions, then a unique global field exists, ensuring total coherence across the glued system.

34:21.600 --> 34:23.200
This guarantees...

34:43.200 --> 34:46.200
...without introducing unexpected thematic breaks or inconsistency.

34:46.200 --> 34:49.200
That makes a lot of sense for assuming things can fit together.

34:49.200 --> 34:52.800
But what happens when the jigsaw puzzle pieces don't quite fit?

34:52.800 --> 34:58.800
Even with sheaves, truly complex collaborations must hit deeper incompatibilities

34:58.800 --> 35:02.800
than not just surface-level contactive complex, but real conceptual mismatches.

35:02.800 --> 35:05.800
That's where stacks and drive categories come in, right?

35:05.800 --> 35:10.600
It sounds like we're moving from making sure things can fit to understanding why they might not fit,

35:10.600 --> 35:14.200
and maybe even how to deal with those more compound mismatches.

35:14.200 --> 35:17.200
Yeah, that's exactly the role they play.

35:17.200 --> 35:20.200
...introduced a bit later, in draft 07 and 08,

35:20.200 --> 35:25.200
specifically to handle complex merge obstructions beyond sheave gluing.

35:25.200 --> 35:27.800
Sheaves are great for ensuring consistency when things do...

35:27.800 --> 35:41.800
...referencing the prerequisites section in DL09 onwards again.

35:41.800 --> 35:44.400
Stacks essentially generalize sheaves.

35:44.400 --> 35:48.400
Instead of assigning just single data points or values to open sets,

35:48.400 --> 35:50.400
like regions in your semantic space,

35:50.400 --> 35:53.400
stacks define entire categories of data to these sets.

35:53.400 --> 35:56.400
And crucially, on the overlaps between these regions,

35:56.400 --> 35:58.400
they don't just require data equality,

35:58.400 --> 36:00.400
they require isomorphisms,

36:00.400 --> 36:03.400
ways of translating between the categories that preserve structure.

36:03.400 --> 36:06.000
This allows them to handle higher cohunitives.

36:06.000 --> 36:08.000
It's not just about whether the data matches,

36:08.000 --> 36:11.000
but whether the structure of the data's relationships matches,

36:11.000 --> 36:13.000
and how those relationships might be equivalent,

36:13.000 --> 36:15.000
even if they look different superficially.

36:15.000 --> 36:17.000
Derived categories, on the other hand,

36:17.000 --> 36:20.000
are a tool borrowed from homological algebra.

36:20.000 --> 36:42.600
The class of incompatibility, why?

36:42.600 --> 36:43.600
Exactly.

36:43.600 --> 36:47.600
This is related with AC1, LM, TM.

36:47.600 --> 36:50.200
As always, what are called first order of instructions.

36:50.200 --> 36:52.400
If the next D1 group is non-zero,

36:52.400 --> 36:54.800
it means there's a fundamental problem in an instruction

36:54.800 --> 36:58.000
that prevents a simple, clean bridge according to the sheet rules.

36:58.000 --> 37:00.600
It signals a genuine incompatibility,

37:00.600 --> 37:04.200
not just a textual conflict, but a deeper conceptual one.

37:04.200 --> 37:06.000
In the RSVP framework,

37:06.000 --> 37:09.400
stacks help to align the A field over these complex overlaps,

37:09.400 --> 37:12.200
even when there are these deeper conceptual difficulties.

37:12.200 --> 37:14.200
The goal is still to minimize entropy,

37:14.200 --> 37:16.200
even in these tricky situations.

37:16.200 --> 37:18.200
The Federated AI Project Anecdote,

37:18.200 --> 37:20.400
which is used from drafts 07 onwards,

37:20.400 --> 37:21.800
provides a great example here.

37:21.800 --> 37:23.800
Imagine AI models trained on diversity.

37:23.800 --> 37:28.400
Do you see the data context?

37:28.400 --> 37:30.800
The stacks then become necessary for resolving these higher expressions,

37:30.800 --> 37:33.000
providing the mathematical machinery to understand,

37:33.000 --> 37:35.000
and potentially reconcile such profound differences,

37:35.000 --> 37:36.000
ensuring that even deeply diffusion models,

37:36.000 --> 37:37.000
perhaps trained on different ontologies of data,

37:37.000 --> 37:38.000
can be integrated meaningfully.

37:38.000 --> 37:42.200
perhaps by finding a higher level structure where they can be seen as compatible.

37:42.200 --> 37:44.200
This is where the math gets really sophisticated then.

37:44.200 --> 37:46.400
It's acknowledging that real-world collaboration

37:46.400 --> 37:50.600
often has these deep, complex incompatibilities that simple tools

37:50.600 --> 37:54.800
just paper over or fail to address entirely.

37:54.800 --> 37:59.000
It's about moving beyond just managing conflicts to actually understanding their mathematical

37:59.000 --> 38:03.200
nature and potentially providing a formal path to their resolution.

38:03.200 --> 38:06.400
It moves us far from a world of just it broke, to potentially it broke because of the specific Conceptualsmith

38:06.400 --> 38:08.300
meme 쯤.

38:08.300 --> 38:14.680
It's acknowledging that real-world collaboration often has these deep, complex incompatibilities

38:14.680 --> 38:18.080
that simple tools just paper over or fail to address entirely.

38:18.540 --> 38:23.060
It's about moving beyond just managing conflicts to actually understanding their mathematical

38:23.060 --> 38:26.920
nature and potentially providing a formal path to their resolution.

38:27.300 --> 38:31.740
It moves us from a world of just it broke to potentially it broke because of this specific

38:31.740 --> 38:35.900
conceptual clash which we can now analyze and perhaps even formally reconcile.

38:35.900 --> 38:40.040
It has a really powerful diagnostic capability for complex systems.

38:40.380 --> 38:44.000
Okay, so we've laid the groundwork with RSVP theory describing the semantic physics and

38:44.000 --> 38:47.640
these advanced mathematical structures, categories, sheaths, stacks describing...

38:47.640 --> 38:55.860
Right, and there's precisely the limitation that the semantic merge operator aims to overcome.

38:55.860 --> 39:12.820
The concept of a sophisticated merge operator is present right from drafts 01, where it's

39:12.820 --> 39:13.860
initially described interestingly as a homotovie-columid-based merge operator, we'll come back to that, but it's in drafts 03 and 04 that it becomes more formalized as 01.

39:13.860 --> 39:31.860
The key difference, as later drafts like D09, D11, D12, and D13 really emphasize in their rationale sections, is this, yet the textual mergers fail to preserve computational intent.

39:31.860 --> 39:36.860
The tool is just merge lines of text, treating code as static data period.

39:36.860 --> 39:43.860
The operator, by contrast, works by aligning the underlying RSVP fields, the UIs, EVs, and S fields.

39:43.860 --> 39:59.860
The bioinformatics project anecdote, which is used consistently from draft 06 onward, illustrates this really well.

39:59.860 --> 40:16.860
Imagine a research team collaborating on integrating two highly complex modules.

40:16.860 --> 40:23.860
One is a cutting edge sequence alignment module, designed to compare DNA sequences and find similarities based on evolutionary models.

40:23.860 --> 40:32.860
The other is a sophisticated 3D protein visualization module, which renders complex molecular structures based on that alignment data.

40:32.860 --> 40:39.860
Now, in a traditional heat environment, if two different people are working on these modules, perhaps refining the alignment algorithm in one branch,

40:39.860 --> 40:45.860
and improving the rendering engine in another, you might easily get textual diffs that lead to conflicts in shared configuration.

40:53.860 --> 41:12.860
The operator, by focusing on aligning these underlying RSVP fields, can recognize this profound complementarity.

41:12.860 --> 41:19.860
It can produce a unified, coherent computational pipeline that works as intended, without forcing the team into frustrating,

41:19.860 --> 41:22.860
potentially error-prone, manual textual reconciliation.

41:22.860 --> 41:28.860
It sees the purpose of each module and images based on that shared goal of analyzing and visualizing biological data.

41:28.860 --> 41:34.860
That's a critical distinction. So it's about discerning the intended function and integrating based on that shooting proof is,

41:34.860 --> 41:37.860
rather than just comparing raw lines of code.

41:37.860 --> 41:40.860
And what about those XT1 obstructions we just talked about with derived categories?

41:40.860 --> 41:44.860
How do they play into whether a semantic merge exceeds or fails?

41:44.860 --> 41:46.860
This is where the mathematical degree really writes.

41:46.860 --> 41:48.860
The Serum C.1.

41:48.860 --> 41:55.860
Merge validity criterion, which appears formally from draft 08 onwards, comes really into play here.

41:55.860 --> 41:59.860
It formally states that a semantic merge, M1, M2, exists if,

41:59.860 --> 42:05.860
and only if that derived category obstruction. XT1, LM, TM, is equal to 0.

42:05.860 --> 42:09.860
If XT1 is non-zero, meaning there's a higher order conceptual incompatibility, a fun-

42:09.860 --> 42:16.860
...

42:16.860 --> 42:20.860
...

42:20.860 --> 42:28.860
...

42:28.860 --> 42:29.860
...

42:29.860 --> 42:30.860
...

42:30.860 --> 42:31.860
...

42:31.860 --> 42:32.860
...

42:32.860 --> 42:33.860
...

42:33.860 --> 42:37.860
You can think of the semantic merge operator as acting like a mediator in a negotiation,

42:37.860 --> 42:41.860
but crucially, along with a deep understanding of the intent and meaning of each party,

42:41.860 --> 42:43.860
the modules are being merged.

42:43.860 --> 42:48.860
If the two modules, or perhaps two teams, with different plans for a project represented by

42:48.860 --> 42:52.860
those modules agree on their shared goals, which corresponds to their overlapping semantic

42:52.860 --> 42:57.860
field of lining properly, then they can successfully combine into a coherent, unified plan,

42:57.860 --> 42:59.860
the merge module M.

42:59.860 --> 43:04.860
If, however, their goals conflict, fundamentally, if there's a non-zero XT1 obstruction indicating

43:04.860 --> 43:09.860
that conceptual clash, then the system doesn't try to force a messy textual...

43:09.860 --> 43:10.860
...

43:10.860 --> 43:11.860
...

43:11.860 --> 43:12.860
...

43:12.860 --> 43:15.860
...

43:15.860 --> 43:16.860
...

43:16.860 --> 43:17.860
...

43:17.860 --> 43:18.860
...

43:18.860 --> 43:19.860
...

43:39.860 --> 43:47.860
...

43:47.860 --> 43:48.860
...

44:09.860 --> 44:10.860
...

44:10.860 --> 44:11.860
...

44:11.860 --> 44:12.860
...

44:12.860 --> 44:13.860
...

44:13.860 --> 44:14.860
...

44:14.860 --> 44:15.860
...

44:15.860 --> 44:16.860
...

44:16.860 --> 44:17.860
...

44:17.860 --> 44:18.860
...

44:18.860 --> 44:19.860
...

44:19.860 --> 44:20.860
...

44:20.860 --> 44:21.860
...

44:21.860 --> 44:22.860
...

44:22.860 --> 44:23.860
...

44:23.860 --> 44:29.860
...

44:29.860 --> 44:30.860
...

44:30.860 --> 44:31.860
...

44:31.860 --> 45:01.860


45:01.860 --> 45:02.860
...

45:02.860 --> 45:03.860
...

45:03.860 --> 45:04.860
...

45:04.860 --> 45:05.860
...

45:05.860 --> 45:35.860


45:35.860 --> 45:36.860
...

45:36.860 --> 45:37.860
...

45:37.860 --> 45:38.860
...

45:38.860 --> 45:39.860
...

45:39.860 --> 45:40.860
...

45:40.860 --> 45:42.860
...

45:42.860 --> 45:43.860
...

45:43.860 --> 46:13.860


46:13.860 --> 46:14.860
...

46:14.860 --> 46:44.860


46:44.860 --> 47:14.860


47:14.860 --> 47:44.860


47:44.860 --> 48:14.860


48:14.860 --> 48:44.860


48:44.860 --> 49:14.860


49:14.860 --> 49:44.860


49:44.860 --> 50:14.860


50:14.860 --> 50:44.860


50:44.860 --> 50:46.860
...

50:46.860 --> 50:47.860
...

50:47.860 --> 50:48.860
...

50:48.860 --> 50:49.860
...

50:49.860 --> 50:50.860
...

50:50.860 --> 50:51.860
...

50:51.860 --> 50:52.860
...

50:52.860 --> 51:22.860


51:22.860 --> 51:52.860


51:52.860 --> 52:22.860


52:22.860 --> 52:52.860


52:52.860 --> 53:22.860


53:22.860 --> 53:52.860


53:52.860 --> 54:22.860


54:22.860 --> 54:52.860


54:52.860 --> 55:22.860


55:22.860 --> 55:52.860


55:52.860 --> 56:22.860


56:22.860 --> 56:52.860


56:52.860 --> 57:22.860


57:22.860 --> 57:23.860
...

57:23.860 --> 57:25.860
...

57:25.860 --> 57:55.860


57:55.860 --> 58:25.860


58:25.860 --> 58:55.860


58:55.860 --> 59:25.860


59:25.860 --> 59:55.860


59:55.860 --> 01:00:25.860


01:00:25.860 --> 01:00:55.860


01:00:55.860 --> 01:00:56.860
...

01:00:56.860 --> 01:01:26.860


01:01:26.860 --> 01:01:56.860


01:01:56.860 --> 01:02:26.860


01:02:26.860 --> 01:02:56.860


01:02:56.860 --> 01:03:26.860


01:03:26.860 --> 01:03:56.860


01:03:56.860 --> 01:04:26.860


01:04:26.860 --> 01:04:56.860


01:04:56.860 --> 01:05:26.860


01:05:26.860 --> 01:05:56.860


01:05:56.860 --> 01:06:26.860


01:06:26.860 --> 01:06:56.860


01:06:56.860 --> 01:07:26.860


01:07:26.860 --> 01:07:56.860


01:07:56.860 --> 01:08:26.860


01:08:26.860 --> 01:08:56.860


01:08:56.860 --> 01:09:26.860


01:09:26.860 --> 01:09:56.860


01:09:56.860 --> 01:10:26.860


01:10:26.860 --> 01:10:56.860


01:10:56.860 --> 01:11:26.860


01:11:26.860 --> 01:11:56.860


01:11:56.860 --> 01:12:26.860


01:12:26.860 --> 01:12:56.860


01:12:56.860 --> 01:13:26.860


01:13:26.860 --> 01:13:56.860


01:13:56.860 --> 01:14:15.110


